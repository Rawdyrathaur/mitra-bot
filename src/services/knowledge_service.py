"""
Advanced knowledge base and search service for SAM Bot
Provides semantic search, multi-language support, and intelligent ranking
"""
import os
import logging
import re
import asyncio
from typing import List, Dict, Optional, Tuple, Set
from datetime import datetime
import json

# Text processing and NLP
try:
    import spacy
    from spacy.lang.en.stop_words import STOP_WORDS as EN_STOP_WORDS
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

try:
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    from nltk.stem import PorterStemmer, WordNetLemmatizer
    import nltk
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

# Language detection
try:
    from langdetect import detect, detect_langs
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False

import numpy as np
from database_manager import DatabaseManager

logger = logging.getLogger(__name__)

class KnowledgeSearchEngine:
    """Advanced knowledge search with semantic similarity and ranking"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db = db_manager
        
        # Initialize NLP components
        self._init_nlp_components()
        
        # Search configuration
        self.similarity_threshold = self.db.get_config('similarity_threshold', 0.1)
        self.max_search_results = self.db.get_config('max_search_results', 10)
        self.context_window = self.db.get_config('conversation_context_window', 5)
        
        # Language support
        self.supported_languages = {
            'en': 'English',
            'es': 'Spanish', 
            'fr': 'French',
            'de': 'German',
            'it': 'Italian',
            'pt': 'Portuguese',
            'ru': 'Russian',
            'zh': 'Chinese',
            'ja': 'Japanese'
        }
        
        logger.info("Knowledge search engine initialized")
    
    def _init_nlp_components(self):
        """Initialize NLP libraries and models"""
        self.nlp = None
        self.stemmer = None
        self.lemmatizer = None
        
        # Initialize spaCy if available
        if SPACY_AVAILABLE:
            try:
                self.nlp = spacy.load("en_core_web_sm")
                logger.info("spaCy English model loaded")
            except OSError:
                logger.warning("spaCy English model not found. Install with: python -m spacy download en_core_web_sm")
        
        # Initialize NLTK if available
        if NLTK_AVAILABLE:
            try:
                nltk.download('stopwords', quiet=True)
                nltk.download('punkt', quiet=True)
                nltk.download('wordnet', quiet=True)
                self.stemmer = PorterStemmer()
                self.lemmatizer = WordNetLemmatizer()
                logger.info("NLTK components initialized")
            except Exception as e:
                logger.warning(f"NLTK initialization failed: {e}")
    
    async def semantic_search(self, query: str, filters: Dict = None, 
                            limit: int = None, include_context: bool = True) -> Dict:
        """Perform comprehensive semantic search with ranking and filtering"""
        try:
            start_time = datetime.utcnow()
            
            # Preprocess query
            processed_query = self._preprocess_query(query)
            
            # Detect query language
            query_language = self._detect_language(query)
            
            # Generate query embedding
            query_embedding = await self._get_query_embedding(processed_query)
            
            # Apply filters
            search_filters = self._build_search_filters(filters)
            
            # Perform vector similarity search
            search_limit = limit or self.max_search_results
            raw_results = self.db.semantic_search(
                query_embedding,
                limit=search_limit * 2,  # Get more results for reranking
                similarity_threshold=self.similarity_threshold
            )\n            \n            # Apply text-based filters and reranking\n            filtered_results = self._apply_filters_and_rerank(\n                raw_results, query, processed_query, search_filters\n            )\n            \n            # Limit final results\n            final_results = filtered_results[:search_limit]\n            \n            # Calculate search metrics\n            processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000\n            \n            response = {\n                'query': query,\n                'language': query_language,\n                'results': final_results,\n                'total_found': len(filtered_results),\n                'processing_time_ms': round(processing_time, 2),\n                'timestamp': start_time.isoformat()\n            }\n            \n            # Add context for conversation if requested\n            if include_context and final_results:\n                response['context_summary'] = self._generate_context_summary(final_results)\n            \n            # Log search analytics\n            self._log_search_analytics(query, len(final_results), processing_time, filters)\n            \n            return response\n            \n        except Exception as e:\n            logger.error(f\"Error in semantic search: {e}\")\n            return {\n                'query': query,\n                'results': [],\n                'error': str(e),\n                'timestamp': datetime.utcnow().isoformat()\n            }\n    \n    def _preprocess_query(self, query: str) -> str:\n        \"\"\"Preprocess query for better search results\"\"\"\n        try:\n            # Basic cleaning\n            processed = query.strip().lower()\n            \n            # Remove extra whitespace\n            processed = re.sub(r'\\s+', ' ', processed)\n            \n            # Expand common abbreviations\n            abbreviations = {\n                'api': 'application programming interface',\n                'faq': 'frequently asked questions',\n                'ai': 'artificial intelligence',\n                'ml': 'machine learning',\n                'ui': 'user interface',\n                'ux': 'user experience'\n            }\n            \n            for abbr, expansion in abbreviations.items():\n                processed = re.sub(rf'\\b{abbr}\\b', f\"{abbr} {expansion}\", processed)\n            \n            return processed\n            \n        except Exception as e:\n            logger.error(f\"Error preprocessing query: {e}\")\n            return query.strip().lower()\n    \n    def _detect_language(self, text: str) -> str:\n        \"\"\"Detect language of input text\"\"\"\n        try:\n            if LANGDETECT_AVAILABLE and len(text) > 10:\n                detected = detect(text)\n                if detected in self.supported_languages:\n                    return detected\n            return 'en'  # Default to English\n        except Exception as e:\n            logger.debug(f\"Language detection failed: {e}\")\n            return 'en'\n    \n    async def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"Generate embedding for search query\"\"\"\n        try:\n            # This should use the same embedding model as document processing\n            from services.document_service import DocumentProcessor\n            \n            # For now, create a simple embedding (in production, use same model as documents)\n            # This is a placeholder - in real implementation, use the actual embedding model\n            embedding_size = 384  # all-MiniLM-L6-v2 size\n            return [0.1] * embedding_size  # Placeholder\n            \n        except Exception as e:\n            logger.error(f\"Error generating query embedding: {e}\")\n            embedding_size = 384\n            return [0.0] * embedding_size\n    \n    def _build_search_filters(self, filters: Dict) -> Dict:\n        \"\"\"Build search filters from request parameters\"\"\"\n        if not filters:\n            return {}\n        \n        search_filters = {}\n        \n        # Document type filter\n        if 'file_type' in filters:\n            search_filters['file_type'] = filters['file_type']\n        \n        # Category filter\n        if 'category' in filters:\n            search_filters['category'] = filters['category']\n        \n        # Date range filter\n        if 'date_from' in filters:\n            search_filters['date_from'] = filters['date_from']\n        if 'date_to' in filters:\n            search_filters['date_to'] = filters['date_to']\n        \n        # Language filter\n        if 'language' in filters:\n            search_filters['language'] = filters['language']\n        \n        return search_filters\n    \n    def _apply_filters_and_rerank(self, results: List[Dict], original_query: str, \n                                processed_query: str, filters: Dict) -> List[Dict]:\n        \"\"\"Apply additional filters and rerank results\"\"\"\n        try:\n            filtered_results = []\n            \n            for result in results:\n                # Apply filters\n                if not self._passes_filters(result, filters):\n                    continue\n                \n                # Calculate additional relevance scores\n                text_match_score = self._calculate_text_relevance(result['content'], processed_query)\n                title_match_score = self._calculate_title_relevance(result['document_title'], processed_query)\n                \n                # Combine scores\n                combined_score = (\n                    result['similarity'] * 0.6 +  # Semantic similarity\n                    text_match_score * 0.3 +      # Text matching\n                    title_match_score * 0.1       # Title relevance\n                )\n                \n                result['combined_score'] = combined_score\n                result['text_match_score'] = text_match_score\n                result['title_match_score'] = title_match_score\n                \n                filtered_results.append(result)\n            \n            # Sort by combined score\n            filtered_results.sort(key=lambda x: x['combined_score'], reverse=True)\n            \n            return filtered_results\n            \n        except Exception as e:\n            logger.error(f\"Error filtering and ranking results: {e}\")\n            return results\n    \n    def _passes_filters(self, result: Dict, filters: Dict) -> bool:\n        \"\"\"Check if result passes all filters\"\"\"\n        try:\n            # TODO: Implement actual filtering based on document metadata\n            # This would require joining with document table to get metadata\n            \n            # For now, just apply similarity threshold\n            return result.get('similarity', 0) >= self.similarity_threshold\n            \n        except Exception as e:\n            logger.error(f\"Error checking filters: {e}\")\n            return True\n    \n    def _calculate_text_relevance(self, content: str, query: str) -> float:\n        \"\"\"Calculate text-based relevance score\"\"\"\n        try:\n            content_lower = content.lower()\n            query_words = query.split()\n            \n            # Count exact word matches\n            word_matches = sum(1 for word in query_words if word in content_lower)\n            \n            # Calculate phrase matches\n            phrase_matches = 1 if query in content_lower else 0\n            \n            # Normalize scores\n            word_score = word_matches / len(query_words) if query_words else 0\n            phrase_score = phrase_matches\n            \n            return min(1.0, word_score * 0.7 + phrase_score * 0.3)\n            \n        except Exception as e:\n            logger.error(f\"Error calculating text relevance: {e}\")\n            return 0.0\n    \n    def _calculate_title_relevance(self, title: str, query: str) -> float:\n        \"\"\"Calculate title relevance score\"\"\"\n        try:\n            title_lower = title.lower()\n            query_lower = query.lower()\n            \n            # Exact title match\n            if query_lower in title_lower:\n                return 1.0\n            \n            # Word-based matching\n            title_words = set(title_lower.split())\n            query_words = set(query_lower.split())\n            \n            if not query_words:\n                return 0.0\n            \n            intersection = title_words & query_words\n            return len(intersection) / len(query_words)\n            \n        except Exception as e:\n            logger.error(f\"Error calculating title relevance: {e}\")\n            return 0.0\n    \n    def _generate_context_summary(self, results: List[Dict]) -> str:\n        \"\"\"Generate a summary of search results for conversation context\"\"\"\n        try:\n            if not results:\n                return \"\"\n            \n            # Take top 3 results and create context\n            context_parts = []\n            for i, result in enumerate(results[:3]):\n                snippet = result['content'][:200] + \"...\" if len(result['content']) > 200 else result['content']\n                context_parts.append(f\"Source {i+1} ({result['document_title']}): {snippet}\")\n            \n            return \"\\n\\n\".join(context_parts)\n            \n        except Exception as e:\n            logger.error(f\"Error generating context summary: {e}\")\n            return \"\"\n    \n    def _log_search_analytics(self, query: str, result_count: int, \n                            processing_time: float, filters: Dict):\n        \"\"\"Log search analytics\"\"\"\n        try:\n            with self.db.get_session() as session:\n                self.db._log_analytics(\n                    session,\n                    'knowledge_search',\n                    metadata={\n                        'query': query,\n                        'result_count': result_count,\n                        'processing_time_ms': processing_time,\n                        'filters_used': bool(filters),\n                        'query_length': len(query)\n                    }\n                )\n        except Exception as e:\n            logger.error(f\"Error logging search analytics: {e}\")\n    \n    async def get_related_documents(self, document_id: str, limit: int = 5) -> List[Dict]:\n        \"\"\"Find documents related to a given document\"\"\"\n        try:\n            with self.db.get_session() as session:\n                from models.database_models import Document, DocumentChunk\n                \n                # Get the document\n                doc = session.query(Document).filter(Document.id == document_id).first()\n                if not doc:\n                    return []\n                \n                # Get representative chunks from the document\n                chunks = session.query(DocumentChunk).filter(\n                    DocumentChunk.document_id == document_id\n                ).order_by(DocumentChunk.chunk_index).limit(3).all()\n                \n                if not chunks:\n                    return []\n                \n                # Use first chunk's embedding to find similar documents\n                first_chunk_embedding = chunks[0].embedding\n                if not first_chunk_embedding:\n                    return []\n                \n                # Search for similar chunks from other documents\n                similar_results = self.db.semantic_search(\n                    first_chunk_embedding,\n                    limit=limit * 3,  # Get more to filter out same document\n                    similarity_threshold=0.3  # Higher threshold for related docs\n                )\n                \n                # Group by document and filter out the original document\n                doc_scores = {}\n                for result in similar_results:\n                    if result['document_id'] != document_id:\n                        if result['document_id'] not in doc_scores:\n                            doc_scores[result['document_id']] = {\n                                'document_id': result['document_id'],\n                                'document_title': result['document_title'],\n                                'category': result['category'],\n                                'max_similarity': result['similarity'],\n                                'match_count': 1\n                            }\n                        else:\n                            doc_scores[result['document_id']]['max_similarity'] = max(\n                                doc_scores[result['document_id']]['max_similarity'],\n                                result['similarity']\n                            )\n                            doc_scores[result['document_id']]['match_count'] += 1\n                \n                # Sort by relevance and return top results\n                related_docs = sorted(\n                    doc_scores.values(),\n                    key=lambda x: (x['max_similarity'], x['match_count']),\n                    reverse=True\n                )[:limit]\n                \n                return related_docs\n                \n        except Exception as e:\n            logger.error(f\"Error finding related documents: {e}\")\n            return []\n    \n    async def advanced_search(self, query: str, search_type: str = 'semantic', \n                            filters: Dict = None, facets: List[str] = None) -> Dict:\n        \"\"\"Advanced search with multiple search strategies\"\"\"\n        try:\n            results = {}\n            \n            if search_type in ['semantic', 'all']:\n                # Semantic search\n                semantic_results = await self.semantic_search(query, filters)\n                results['semantic'] = semantic_results\n            \n            if search_type in ['keyword', 'all']:\n                # Keyword search\n                keyword_results = await self._keyword_search(query, filters)\n                results['keyword'] = keyword_results\n            \n            if search_type in ['fuzzy', 'all']:\n                # Fuzzy search for handling typos\n                fuzzy_results = await self._fuzzy_search(query, filters)\n                results['fuzzy'] = fuzzy_results\n            \n            # Combine results if multiple search types\n            if search_type == 'all':\n                combined_results = self._combine_search_results(results)\n                results['combined'] = combined_results\n            \n            # Add facets if requested\n            if facets:\n                results['facets'] = await self._generate_facets(results.get('semantic', {}).get('results', []), facets)\n            \n            return results\n            \n        except Exception as e:\n            logger.error(f\"Error in advanced search: {e}\")\n            return {'error': str(e)}\n    \n    async def _keyword_search(self, query: str, filters: Dict = None) -> Dict:\n        \"\"\"Keyword-based search using full-text search\"\"\"\n        try:\n            # This is a simplified version - in production, use PostgreSQL full-text search\n            with self.db.get_session() as session:\n                from models.database_models import DocumentChunk, Document\n                from sqlalchemy import or_, and_\n                \n                # Build keyword search query\n                keywords = query.split()\n                \n                # Create LIKE clauses for each keyword\n                like_clauses = []\n                for keyword in keywords:\n                    like_clauses.append(DocumentChunk.content.ilike(f'%{keyword}%'))\n                \n                # Execute search\n                query_obj = session.query(\n                    DocumentChunk.id,\n                    DocumentChunk.document_id,\n                    DocumentChunk.content,\n                    Document.title,\n                    Document.category\n                ).join(Document).filter(\n                    and_(*like_clauses)  # All keywords must be present\n                ).order_by(\n                    func.length(DocumentChunk.content)  # Prefer shorter, more focused chunks\n                ).limit(self.max_search_results)\n                \n                results = query_obj.all()\n                \n                formatted_results = []\n                for chunk_id, doc_id, content, title, category in results:\n                    # Calculate simple keyword relevance\n                    keyword_score = self._calculate_keyword_relevance(content, keywords)\n                    \n                    formatted_results.append({\n                        'chunk_id': chunk_id,\n                        'document_id': doc_id,\n                        'content': content,\n                        'document_title': title,\n                        'category': category,\n                        'keyword_score': keyword_score,\n                        'search_type': 'keyword'\n                    })\n                \n                return {\n                    'query': query,\n                    'results': formatted_results,\n                    'total_found': len(formatted_results)\n                }\n                \n        except Exception as e:\n            logger.error(f\"Error in keyword search: {e}\")\n            return {'query': query, 'results': [], 'error': str(e)}\n    \n    async def _fuzzy_search(self, query: str, filters: Dict = None) -> Dict:\n        \"\"\"Fuzzy search for handling typos and variations\"\"\"\n        try:\n            # This is a simplified implementation\n            # In production, you'd use more sophisticated fuzzy matching\n            \n            # Generate query variations\n            variations = self._generate_query_variations(query)\n            \n            # Search for each variation\n            all_results = []\n            for variation in variations:\n                var_results = await self.semantic_search(variation, filters, limit=5)\n                for result in var_results.get('results', []):\n                    result['search_type'] = 'fuzzy'\n                    result['query_variation'] = variation\n                    all_results.append(result)\n            \n            # Deduplicate by chunk_id\n            seen_chunks = set()\n            unique_results = []\n            for result in all_results:\n                chunk_id = result.get('chunk_id')\n                if chunk_id not in seen_chunks:\n                    seen_chunks.add(chunk_id)\n                    unique_results.append(result)\n            \n            # Sort by similarity\n            unique_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)\n            \n            return {\n                'query': query,\n                'results': unique_results[:self.max_search_results],\n                'total_found': len(unique_results)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error in fuzzy search: {e}\")\n            return {'query': query, 'results': [], 'error': str(e)}\n    \n    def _generate_query_variations(self, query: str) -> List[str]:\n        \"\"\"Generate query variations for fuzzy search\"\"\"\n        variations = [query]\n        \n        try:\n            # Add stemmed version\n            if self.stemmer:\n                stemmed_words = [self.stemmer.stem(word) for word in query.split()]\n                variations.append(\" \".join(stemmed_words))\n            \n            # Add synonyms (simplified - in production use WordNet or custom synonym dict)\n            synonyms = {\n                'help': ['assistance', 'support', 'aid'],\n                'problem': ['issue', 'trouble', 'difficulty'],\n                'setup': ['configuration', 'installation', 'setup'],\n                'error': ['issue', 'problem', 'bug']\n            }\n            \n            for word, syns in synonyms.items():\n                if word in query.lower():\n                    for syn in syns:\n                        var_query = re.sub(rf'\\b{word}\\b', syn, query, flags=re.IGNORECASE)\n                        if var_query != query:\n                            variations.append(var_query)\n            \n            return list(set(variations))  # Remove duplicates\n            \n        except Exception as e:\n            logger.error(f\"Error generating query variations: {e}\")\n            return [query]\n    \n    def _calculate_keyword_relevance(self, content: str, keywords: List[str]) -> float:\n        \"\"\"Calculate keyword-based relevance score\"\"\"\n        try:\n            content_lower = content.lower()\n            total_matches = 0\n            \n            for keyword in keywords:\n                # Count occurrences of each keyword\n                matches = content_lower.count(keyword.lower())\n                total_matches += matches\n            \n            # Normalize by content length and keyword count\n            content_words = len(content.split())\n            if content_words == 0:\n                return 0.0\n            \n            # Simple TF-like score\n            score = total_matches / content_words\n            return min(1.0, score * 10)  # Scale and cap at 1.0\n            \n        except Exception as e:\n            logger.error(f\"Error calculating keyword relevance: {e}\")\n            return 0.0\n    \n    def _combine_search_results(self, results_dict: Dict) -> Dict:\n        \"\"\"Combine results from multiple search strategies\"\"\"\n        try:\n            all_results = []\n            \n            # Collect all results\n            for search_type, search_result in results_dict.items():\n                if isinstance(search_result, dict) and 'results' in search_result:\n                    for result in search_result['results']:\n                        result['search_type'] = search_type\n                        all_results.append(result)\n            \n            # Deduplicate by chunk_id and combine scores\n            combined_results = {}\n            for result in all_results:\n                chunk_id = result.get('chunk_id')\n                if chunk_id not in combined_results:\n                    combined_results[chunk_id] = result\n                    combined_results[chunk_id]['search_methods'] = [result.get('search_type', 'unknown')]\n                else:\n                    # Combine scores from multiple search methods\n                    existing = combined_results[chunk_id]\n                    existing['search_methods'].append(result.get('search_type', 'unknown'))\n                    \n                    # Take best similarity score\n                    existing['similarity'] = max(\n                        existing.get('similarity', 0),\n                        result.get('similarity', 0)\n                    )\n            \n            # Convert back to list and sort\n            final_results = list(combined_results.values())\n            final_results.sort(key=lambda x: (\n                len(x['search_methods']),  # Prefer results found by multiple methods\n                x.get('similarity', 0)     # Then by similarity\n            ), reverse=True)\n            \n            return {\n                'query': results_dict.get('semantic', {}).get('query', ''),\n                'results': final_results[:self.max_search_results],\n                'total_found': len(final_results),\n                'search_methods_used': list(results_dict.keys())\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error combining search results: {e}\")\n            return {'results': [], 'error': str(e)}\n    \n    async def _generate_facets(self, results: List[Dict], requested_facets: List[str]) -> Dict:\n        \"\"\"Generate search facets for filtering\"\"\"\n        try:\n            facets = {}\n            \n            # Count facet values\n            for facet in requested_facets:\n                facet_counts = {}\n                \n                for result in results:\n                    if facet == 'category':\n                        value = result.get('category', 'unknown')\n                    elif facet == 'document_type':\n                        # Would need to join with document table to get file_type\n                        value = 'unknown'  # Placeholder\n                    else:\n                        continue\n                    \n                    facet_counts[value] = facet_counts.get(value, 0) + 1\n                \n                facets[facet] = dict(sorted(facet_counts.items(), key=lambda x: x[1], reverse=True))\n            \n            return facets\n            \n        except Exception as e:\n            logger.error(f\"Error generating facets: {e}\")\n            return {}\n    \n    def get_search_suggestions(self, partial_query: str, limit: int = 5) -> List[str]:\n        \"\"\"Get search suggestions for autocomplete\"\"\"\n        try:\n            if len(partial_query) < 2:\n                return []\n            \n            # This is a simplified implementation\n            # In production, you'd maintain a separate suggestions index\n            \n            suggestions = []\n            \n            # Common search patterns\n            common_queries = [\n                \"How to setup\",\n                \"How to configure\",\n                \"What is\",\n                \"How does\",\n                \"Troubleshoot\",\n                \"Error message\",\n                \"Installation guide\",\n                \"User manual\"\n            ]\n            \n            partial_lower = partial_query.lower()\n            for query_template in common_queries:\n                if query_template.lower().startswith(partial_lower):\n                    suggestions.append(query_template)\n            \n            # Add document titles that match\n            with self.db.get_session() as session:\n                from models.database_models import Document\n                \n                docs = session.query(Document.title).filter(\n                    Document.title.ilike(f'%{partial_query}%')\n                ).limit(limit).all()\n                \n                for (title,) in docs:\n                    if len(suggestions) < limit:\n                        suggestions.append(title)\n            \n            return suggestions[:limit]\n            \n        except Exception as e:\n            logger.error(f\"Error getting search suggestions: {e}\")\n            return []\n    \n    def get_search_analytics(self, days: int = 7) -> Dict:\n        \"\"\"Get search analytics for the specified period\"\"\"\n        try:\n            with self.db.get_session() as session:\n                from datetime import timedelta\n                from sqlalchemy import func\n                from models.database_models import Analytics\n                \n                since = datetime.utcnow() - timedelta(days=days)\n                \n                # Get search volume\n                search_volume = session.query(func.count(Analytics.id)).filter(\n                    Analytics.event_type == 'knowledge_search',\n                    Analytics.timestamp >= since\n                ).scalar()\n                \n                # Get average processing time\n                avg_time = session.query(\n                    func.avg(Analytics.metadata['processing_time_ms'].astext.cast(Float))\n                ).filter(\n                    Analytics.event_type == 'knowledge_search',\n                    Analytics.timestamp >= since,\n                    Analytics.metadata.has_key('processing_time_ms')\n                ).scalar()\n                \n                # Get top queries (simplified)\n                # In production, you'd extract queries from metadata\n                \n                return {\n                    'period_days': days,\n                    'total_searches': search_volume or 0,\n                    'avg_processing_time_ms': float(avg_time) if avg_time else 0,\n                    'search_trends': []  # Would implement trending queries analysis\n                }\n                \n        except Exception as e:\n            logger.error(f\"Error getting search analytics: {e}\")\n            return {}\n\nclass KnowledgeManagement:\n    \"\"\"Knowledge base management with versioning and organization\"\"\"\n    \n    def __init__(self, db_manager: DatabaseManager, search_engine: KnowledgeSearchEngine):\n        self.db = db_manager\n        self.search = search_engine\n        \n    def get_categories(self) -> List[Dict]:\n        \"\"\"Get all document categories with counts\"\"\"\n        try:\n            with self.db.get_session() as session:\n                from models.database_models import Document\n                from sqlalchemy import func\n                \n                categories = session.query(\n                    Document.category,\n                    func.count(Document.id).label('count')\n                ).filter(\n                    Document.processing_status == 'completed'\n                ).group_by(Document.category).all()\n                \n                return [{\n                    'name': cat.category or 'uncategorized',\n                    'count': cat.count\n                } for cat in categories]\n                \n        except Exception as e:\n            logger.error(f\"Error getting categories: {e}\")\n            return []\n    \n    def get_knowledge_stats(self) -> Dict:\n        \"\"\"Get knowledge base statistics\"\"\"\n        try:\n            with self.db.get_session() as session:\n                from models.database_models import Document, DocumentChunk\n                from sqlalchemy import func\n                \n                # Document counts by status\n                doc_stats = session.query(\n                    Document.processing_status,\n                    func.count(Document.id).label('count')\n                ).group_by(Document.processing_status).all()\n                \n                # Total chunks\n                total_chunks = session.query(func.count(DocumentChunk.id)).scalar()\n                \n                # Average chunk size\n                avg_chunk_size = session.query(func.avg(DocumentChunk.char_count)).scalar()\n                \n                # Recent uploads (last 7 days)\n                from datetime import timedelta\n                recent_cutoff = datetime.utcnow() - timedelta(days=7)\n                recent_uploads = session.query(func.count(Document.id)).filter(\n                    Document.created_at >= recent_cutoff\n                ).scalar()\n                \n                return {\n                    'total_documents': sum(stat.count for stat in doc_stats),\n                    'documents_by_status': {stat.processing_status: stat.count for stat in doc_stats},\n                    'total_chunks': total_chunks or 0,\n                    'avg_chunk_size': int(avg_chunk_size) if avg_chunk_size else 0,\n                    'recent_uploads': recent_uploads or 0\n                }\n                \n        except Exception as e:\n            logger.error(f\"Error getting knowledge stats: {e}\")\n            return {}
